# Testing with Speconsense-Synth

This guide explains how to use `speconsense-synth` for empirical testing of consensus quality, variant detection, and understanding the behavior of the Nanopore + Specimux + Speconsense toolchain.

## What is Speconsense-Synth?

`speconsense-synth` is a synthetic read generator that creates simulated Oxford Nanopore reads from reference sequences with controlled error rates. It allows you to generate test datasets where you know the ground truth, enabling systematic exploration of how the bioinformatics pipeline handles different scenarios.

This is an exploratory tool designed to help users and developers build confidence in results by empirically testing specific hypotheses about consensus generation, variant calling, and artifact detection. While it cannot answer all questions definitively (real biological data is complex), it provides valuable bounds on expected behavior and helps identify potential issues.

## Questions Speconsense-Synth Can Help Answer

The reality of variant detection and consensus quality will only emerge over time as we apply the toolchain to large numbers of specimens and patterns become clear. In the meantime, `speconsense-synth` allows us to investigate important questions through controlled experiments:

**Reliability and Quality**
- How reliable are the consensus sequences generated by the Nanopore + Specimux + Speconsense pipeline?
- Given Nanopore's relatively higher per-base error rate, but assuming errors are randomly distributed, how serious is this problem in practice?
- How many reads do we need to get a reliable consensus given a certain error rate?
- What do the stability metrics (p50diff, p95diff) tell us about consensus quality?

**Variant Detection**
- Are all variants identified by Speconsense biologically real, or are some of them bioinformatic artifacts?
- When Speconsense produces many variants for a single specimen that differ only slightly or are intermediate between each other, are these intermediates just artifacts created by mixing reads from a few biologically real variants?
- How do different clustering algorithms (MCL vs greedy) handle variant separation?

**Contamination and Mixing**
- What does a naive consensus combining reads from two different biological variants look like?
- What does a consensus contaminated with reads from laboratory or bioinformatic contamination look like?
- At what contamination levels does clustering successfully separate sequences vs create "franken-consensuses"?

These experiments provide confidence intervals and error bars on current methods while we wait for real-world patterns to emerge from production data.

## Understanding Nanopore Error Characteristics

To design realistic synthetic tests, it's important to understand typical error rates and patterns in Oxford Nanopore sequencing.

### Current Technology (R10.4.1 + V14 Chemistry)

Modern Oxford Nanopore technology achieves:
- **Raw read accuracy**: ~99% (marketed as "Q20+ chemistry")
- **Overall error rate**: ~1% per base
- **With UMI-based error correction**: Up to 99.99% accuracy (0.01% error rate)

For **realistic testing with modern chemistry**, use error rates of **0.01-0.02** (1-2%).

### Older Technology (R9.4 and Earlier)

Historical Nanopore sequencing had higher error rates:
- **R9.4 chemistry**: 85-94% accuracy (average 92%), meaning **6-8% error rate**
- **2019-era MinION**: 6-7% error rate with high-accuracy basecalling
- **Default Q10 filtering**: Automatically discards sequences with >10% error rate

For **conservative testing or older data**, use error rates of **0.05-0.10** (5-10%).

### Error Type Distribution

Nanopore errors are not evenly distributed across error types:
- **Deletions**: Most common (~51% of errors), overall rate 3-4%
- **Insertions**: ~22% of errors, overall rate 2-4.5%
- **Substitutions**: ~27% of errors, overall rate 2.5-4%

**Important patterns**:
- Homopolymer regions and short tandem repeats cause ~50% of all sequencing errors
- GC content affects error rates: low-GC regions ~6%, high-GC regions ~8%
- Deletions significantly outnumber other error types

**Note**: `speconsense-synth` currently distributes error types equally (⅓ each: insertion, deletion, substitution). This is simpler than reality but provides a reasonable first approximation for testing. Real Nanopore data will have more deletions and position-specific biases.

### Implications for Testing

When designing synthetic experiments:
1. **Modern workflows**: Test with 0.01-0.02 error rates to match current R10.4.1 chemistry
2. **Conservative bounds**: Use 0.05-0.10 to stress-test the pipeline or match older chemistry
3. **Error distribution**: Remember real data has more deletions and homopolymer issues than synth generates
4. **Random vs systematic**: Synth errors are random; real errors may be systematic (same position across reads)

## Worked Examples

### 1. Error Rate Impact on Consensus Quality

**Question**: How do different error rates affect consensus quality and stability metrics?

**Setup**: Generate synthetic reads from a single reference at varying error rates and examine the resulting consensus quality.

```bash
# Create a reference sequence (or use existing FASTA)
cat > reference.fasta <<EOF
>ITS_reference
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
GTTTCTATATGTAACTTCTGAGTAAAACCATAAATAAATCAAAACTTTCAACAACGGATCTCTTGGT
EOF

# Generate reads at different error rates
speconsense-synth reference.fasta -n 200 -e 0.01 --seed 42 -o reads_1pct.fastq
speconsense-synth reference.fasta -n 200 -e 0.05 --seed 42 -o reads_5pct.fastq
speconsense-synth reference.fasta -n 200 -e 0.10 --seed 42 -o reads_10pct.fastq

# Run speconsense on each dataset
speconsense reads_1pct.fastq -o test_1pct --min-size 0
speconsense reads_5pct.fastq -o test_5pct --min-size 0
speconsense reads_10pct.fastq -o test_10pct --min-size 0

# Examine the consensus sequences and stability metrics
grep "^>" test_1pct/reads_1pct-all.fasta
grep "^>" test_5pct/reads_5pct-all.fasta
grep "^>" test_10pct/reads_10pct-all.fasta
```

**What to look for**:
- **1% error rate**: Should produce a single consensus with excellent stability (p50diff=0, p95diff close to 0)
- **5% error rate**: May show elevated p95diff values, indicating some subsampling variation
- **10% error rate**: Higher stability metrics, potential for spurious variant clusters

**Observed in practice**:
Using real fungal ITS sequences with N=200 reads:
- **1-2% error** (modern R10.4.1 chemistry): 100% consensus identity, p50diff=0, p95diff=0
- **5% error** (conservative/older chemistry): 99.85% consensus identity, p50diff=0, p95diff=1
- **10% error** (stress test): 99.85% consensus identity, p50diff=0, p95diff=1

Even at 10% error rate, consensus quality remains above the 99.5% biological match threshold. Modern error rates (1-2%) produce essentially perfect consensus sequences.

**Interpretation**:
- Low p50diff values (0-1) indicate the consensus is stable across subsampling
- Elevated p50diff (≥2) suggests systematic heterogeneity in the read pool
- Compare consensus sequence to reference to measure accuracy
- Count the number of variants: multiple clusters from a single reference indicate error-driven over-clustering

**Example expected output**:
```
# 1% error rate - excellent quality
>reads_1pct-c1 size=200 ric=100 p50diff=0.0 p95diff=0.0

# 5% error rate - good quality, some variation
>reads_5pct-c1 size=195 ric=100 p50diff=0.0 p95diff=2.0
>reads_5pct-c2 size=5 ric=5 p50diff=1.0 p95diff=3.0

# 10% error rate - degraded quality, possible spurious variants
>reads_10pct-c1 size=180 ric=100 p50diff=1.0 p95diff=4.0
>reads_10pct-c2 size=15 ric=15 p50diff=2.0 p95diff=5.0
>reads_10pct-c3 size=5 ric=5 p50diff=3.0 p95diff=6.0
```

This experiment helps you understand: (1) what stability metrics mean in practice, (2) how error rates affect clustering behavior, and (3) what quality thresholds are reasonable for your data.

### 2. Detecting Artifact Variants from Mixed Populations

**Question**: When reads from multiple biological variants are present, does Speconsense create intermediate "phantom" variants that don't actually exist biologically?

**Setup**: Create two variants differing by 2-3 SNPs, mix their reads, and look for intermediate sequences.

```bash
# Create two variants that differ at specific positions
cat > two_variants.fasta <<EOF
>variant_A
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
>variant_B
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTGAAACGGGACGGCCCGCCGGAGGACCCCTAAACTCT
EOF
# variant_B differs at 2 positions (marked above with differences)

# Mix variants 50/50 with different error rates
speconsense-synth two_variants.fasta -n 400 -e 0.02 --ratios 50,50 --seed 42 -o mixed_2pct.fastq
speconsense-synth two_variants.fasta -n 400 -e 0.05 --ratios 50,50 --seed 42 -o mixed_5pct.fastq
speconsense-synth two_variants.fasta -n 400 -e 0.10 --ratios 50,50 --seed 42 -o mixed_10pct.fastq

# Cluster with both algorithms
# Note: Use --min-cluster-ratio 0 to see ALL clusters including small artifacts
speconsense mixed_2pct.fastq --algorithm graph -o mcl_2pct --min-size 0 --min-cluster-ratio 0
speconsense mixed_2pct.fastq --algorithm greedy -o greedy_2pct --min-size 0 --min-cluster-ratio 0

# Test higher error rates with MCL
speconsense mixed_5pct.fastq --algorithm graph -o mcl_5pct --min-size 0 --min-cluster-ratio 0
speconsense mixed_10pct.fastq --algorithm graph -o mcl_10pct --min-size 0 --min-cluster-ratio 0

# Examine the output
grep "^>" mcl_2pct/mixed_2pct-all.fasta

# Run summarize to see if SNP merging creates the expected merged sequence
speconsense-summarize --source mcl_2pct --summary-dir mcl_2pct_summary --min-ric 5
```

**What to look for**:
- **Expected**: Two main clusters corresponding to variants A and B
- **Artifact detection**: Check for small intermediate clusters with sequences that have one SNP from A and one from B (phantom variants)
- **Algorithm comparison**: MCL may produce more granular variants; greedy may merge more aggressively
- **SNP merging**: `speconsense-summarize` may merge the two variants if they differ by ≤2 SNPs (controlled by `--merge-position-count`)

**Observed in practice**:
Testing with closely-related *Amanita* variants (99.5% identity, 3 SNPs apart) at 2% error:
- **MCL**: Successfully separates both variants (100% identity to each reference). Additional small clusters (size ≤4) are sequencing error artifacts, NOT biological intermediates from mixing.
- **Greedy**: Algorithm behavior depends on mixing ratio:
  - 50/50 mix: Creates a franken-consensus (see Example 4 for details)
  - Unequal ratios: May separate or merge depending on distance threshold

**Key finding on artifacts**: When examining clusters with `--min-cluster-ratio 0`, ALL small clusters (≤4 reads) were sequencing error artifacts, clearly matching one parent variant or the other (~99% to one, ~92% to the other). None showed intermediate identity suggesting biological mixing.

**Interpretation**:
- Intermediate variants with small size (RiC <20) are likely artifacts from error-driven mixing
- Intermediate variants with large size suggest real heterogeneity or contamination
- Higher error rates increase artifact formation
- Use `--min-size` and `--min-cluster-ratio` to filter small artifact clusters
- Check merged sequences in summary output: if variants A and B merge into one IUPAC consensus, this represents the phasing loss discussed in [Understanding RiC and Merging](understanding-ric-and-merging.md)

**Example investigation**:
```bash
# Extract consensus sequences and align to reference variants
# Look for intermediate sequences (one SNP from each parent)
# Count cluster sizes to identify likely artifacts vs real variants

# Check if speconsense-summarize merged them
grep "rawric" mcl_2pct_summary/*.fasta
# Output like: >sample-1 size=400 ric=200 rawric=200+200 snp=2
# This indicates two variants merged with IUPAC codes (ambiguity)
```

This experiment demonstrates the phasing problem and helps calibrate your expectation for when variants are real vs artifacts.

### 3. Read Depth Requirements for Reliable Consensus

**Question**: How many reads are needed to generate a reliable consensus sequence (≥99.5% identity to reference) at a given error rate?

**Setup**: Generate varying numbers of reads from a single reference and measure consensus identity. Use granular stepping at low read counts to find the exact threshold where consensus becomes reliable.

```bash
# Create reference
cat > reference.fasta <<EOF
>test_sequence
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
EOF

# Test at different error rates with granular N stepping
# Modern chemistry (1-2% error) - test very low N
for N in 2 3 4 5 6 7 8 9 10 15 20; do
    speconsense-synth reference.fasta -n $N -e 0.02 --seed 42 -o reads_1pct_n${N}.fastq
    speconsense reads_1pct_n${N}.fastq -o test_1pct_n${N} --min-size 0
done

# Older chemistry (5% error) - test low to medium N
for N in 2 3 4 5 6 7 8 9 10 15 20 30 50; do
    speconsense-synth reference.fasta -n $N -e 0.05 --seed 42 -o reads_5pct_n${N}.fastq
    speconsense reads_5pct_n${N}.fastq -o test_5pct_n${N} --min-size 0
done

# Extreme error (10%) - test higher N
for N in 20 30 40 50 75 100; do
    speconsense-synth reference.fasta -n $N -e 0.10 --seed 42 -o reads_10pct_n${N}.fastq
    speconsense reads_10pct_n${N}.fastq -o test_10pct_n${N} --min-size 0
done

# Compare consensus identity to reference
# (Use edlib or similar alignment tool to calculate % identity)
```

**What to look for**:
- **Threshold N:** At what read count does consensus identity cross ≥99.5%?
- **Perfect consensus:** At what N does identity reach 100%?
- **Failure modes:** Can consensus be generated at very low N (N<5)?
- **Error rate effect:** How much does error rate change the minimum N requirement?

**Observed in practice**:

Testing with *Russula* sp. ITS sequence (599 bp) using granular N stepping from 1 to 100:

**Modern chemistry (1-2% error):**
- **N=1:** No consensus (cannot cluster single read)
- **N=2:** 99.33% identity (just below threshold)
- **N=3:** 99.83% identity ✓ (crosses 99.5% threshold)
- **N≥4:** 100% identity (perfect consensus)

**Older chemistry (5% error):**
- **N=1:** No consensus
- **N=2:** 96.56% identity (poor quality)
- **N=3:** 99.00% identity (close but below threshold)
- **N=4:** 99.67% identity ✓ (crosses 99.5% threshold)
- **N≥7:** 100% identity consistently

**Extreme error (10%):**
- **N≤15:** No consensus (clustering fails, reads too divergent)
- **N=20:** 91.08% identity (consensus generated but poor quality)
- **N=30:** 97.84% identity (approaching threshold)
- **N=40:** 99.83% identity ✓ (crosses 99.5% threshold)
- **N≥50:** 100% identity

**Key finding:** With modern ONT chemistry (1-2% error), **as few as 3-4 reads** can produce a reliable consensus (≥99.5% identity). This is much lower than previous estimates based on stability metrics.

**Read depth for different purposes:**

Reliable consensus and other quality metrics have **different read depth requirements**:

| Purpose | Minimum N | Reason |
|---------|-----------|--------|
| Reliable consensus (≥99.5% identity) | 3-4 | Error correction via voting |
| Stability calculation (p50diff, p95diff) | 20+ | Subsampling needs sufficient reads |
| Detect 10% contamination | 50+ | Needs ≥5 minority reads |
| Detect 5% contamination | 100+ | Needs ≥5 minority reads |

**Implication:** A specimen with RiC=10 might have:
- ✓ Reliable consensus of majority population (100% identity)
- ✗ Insufficient reads for stability assessment (p50diff unavailable or unreliable)
- ✗ Insufficient for contamination detection below 20%

This explains why single clusters don't guarantee clean specimens - contamination may be present but below detection threshold!

See Example 4 for details on contamination detection across read depths.

**Interpretation**:
- **RiC <3:** Consensus unreliable, discard
- **RiC 3-5:** Consensus likely reliable (modern chemistry) but no stability metrics, limited contamination detection
- **RiC 5-20:** Good consensus quality, marginal stability assessment, detects >20% contamination
- **RiC 20-50:** Reliable consensus + stable metrics, detects >10% contamination
- **RiC ≥50:** Excellent quality, good contamination detection (≥10%)
- **RiC ≥100:** Sensitive contamination detection (≥5%)

**Expected results summary**:

```
Read Depth vs Consensus Identity

   N |   1% error  |   5% error  |  10% error
-----|-------------|-------------|-------------
   2 |   99.33%    |   96.56%    |    FAIL
   3 | ✓ 99.83%    |   99.00%    |    FAIL
   4 | ✓ 100%      | ✓ 99.67%    |    FAIL
   5 | ✓ 100%      | ✓ 99.83%    |    FAIL
   7 | ✓ 100%      | ✓ 100%      |    FAIL
  10 | ✓ 100%      | ✓ 100%      |    FAIL
  20 | ✓ 100%      | ✓ 100%      |   91.08%
  30 | ✓ 100%      | ✓ 100%      |   97.84%
  40 | ✓ 100%      | ✓ 100%      | ✓ 99.83%
  50 | ✓ 100%      | ✓ 99.83%    | ✓ 100%
 100 | ✓ 100%      | ✓ 99.83%    | ✓ 100%

Minimum N for ≥99.5% identity:
  1% error: N ≥ 3 reads
  5% error: N ≥ 4 reads
 10% error: N ≥ 40 reads
```

This experiment helps you:
1. Understand the remarkably low read requirements for reliable consensus (3-4 reads for modern chemistry)
2. Recognize that consensus quality and other purposes (stability, contamination detection) have different read depth needs
3. Set appropriate RiC filtering thresholds based on intended use
4. Understand why low-RiC specimens can have good consensus but limited contamination detection

### 4. Contamination Scenarios

**Question**: What happens when reads from a contaminant sequence mix with the target sequence? Do they cluster separately or create a franken-consensus?

**Setup**: Mix a target sequence with a contaminant at varying ratios and examine clustering behavior.

```bash
# Create target and contaminant sequences (e.g., similar but distinct species)
cat > target_and_contaminant.fasta <<EOF
>target_species
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
>contaminant
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
GTGAACATACCTAATGTTGCCTCGGCGGATCAGCCCGCCCGGCCCGCCAGAGGACCCCTAAACTCTGT
EOF
# Contaminant differs significantly (different species/genus)

# Test different contamination levels
# 95/5 ratio - minor contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5.fastq

# 90/10 ratio - moderate contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 90,10 --seed 42 -o contam_90_10.fastq

# 80/20 ratio - heavy contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 80,20 --seed 42 -o contam_80_20.fastq

# Test with both clustering algorithms
# Note: Use --min-cluster-ratio 0 to see ALL clusters including small artifacts
speconsense contam_95_5.fastq --algorithm graph -o mcl_95_5 --min-size 0 --min-cluster-ratio 0
speconsense contam_95_5.fastq --algorithm greedy -o greedy_95_5 --min-size 0 --min-cluster-ratio 0

# Test additional ratios
speconsense contam_90_10.fastq --algorithm graph -o mcl_90_10 --min-size 0 --min-cluster-ratio 0
speconsense contam_80_20.fastq --algorithm graph -o mcl_80_20 --min-size 0 --min-cluster-ratio 0

# Compare algorithm behavior at equal mixture (50/50)
# This tests the franken-consensus danger zone
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 50,50 --seed 42 -o contam_50_50.fastq
speconsense contam_50_50.fastq --algorithm graph -o mcl_50_50 --min-size 0 --min-cluster-ratio 0
speconsense contam_50_50.fastq --algorithm greedy -o greedy_50_50 --min-size 0 --min-cluster-ratio 0
```

**What to look for**:
- **Separation**: Do you get two clean clusters (target + contaminant)?
- **Cluster sizes**: Do they match expected ratios (e.g., ~380 vs ~20 for 95/5)?
- **Franken-consensus**: At what contamination level do sequences start mixing into chimeric consensuses?
- **Algorithm differences**: Does graph (MCL) separate better than greedy?
- **Small artifacts**: Do you see tiny clusters (size ≤4) that are error artifacts rather than real variants?
- **Read depth effects**: How does total read count affect detection of minority population?

**Observed in practice**:

Testing contamination scenarios reveals distinct algorithm behaviors and critical detection limits:

**1. Distantly-related contamination (e.g., different genera at ~65% identity)**:
- MCL detects cleanly at all tested ratios (95/5, 90/10, 80/20)
- Both consensuses achieve 99.85-100% identity to references
- Some error artifact clusters (size ≤4) appear, especially at higher contamination
- Greedy behavior varies by ratio (see below)

**2. Closely-related contamination (~92% identity, same genus)**:
This is the challenging "worst case" scenario revealing critical algorithm differences:

**MCL Algorithm (graph-based):**
- ✓ Successfully detects both variants at all contamination levels **when minority has ≥5 reads**
- ✓ All main consensuses: 99.66-100% identity to references
- ✓ Read depth requirements for detection:
  - 5% contamination: Requires N≥100 (to yield 5 minority reads)
  - 10% contamination: Requires N≥50 (to yield 5 minority reads)
  - 20% contamination: Detectable at N≥25
- **Critical finding**: Detection depends on **absolute minority read count (~5 minimum)**, not just percentage

**Greedy Algorithm:**
Behavior changes dramatically based on contamination level:

- **At ~50% contamination (equal mixture)**:
  - Creates **franken-consensus** (e.g., 96.40% identity when both parents are >99%)
  - **WARNING SIGNAL**: Elevated p50diff (e.g., 18.0) indicates heterogeneity
  - Sequence falls below 99.5% threshold → would fail quality checks
  - This is detectable (doesn't pass as good data)

- **At <20% contamination (realistic contamination levels)**:
  - Creates perfect **majority consensus** (100% identity to majority variant)
  - **NO WARNING SIGNAL**: p50diff=0, appears perfectly stable
  - Minority population completely ignored
  - Would pass all quality checks despite contamination present

- **Transition zone at ~20-40% contamination**: Behavior is unpredictable

**3. Most Important Characteristic - When Artifacts Appear in Consensus**:

Across all tested scenarios, **artifacts appearing in the consensus sequence** only occurred with:
- Greedy algorithm at ~50% contamination of closely-related sequences
- This creates detectable franken-consensus (high p50diff, low identity)

**Silent failures** (contamination present but consensus appears clean):
- Greedy at <20% contamination: Creates clean majority consensus, no errors, no warnings
- MCL below detection threshold (<5 minority reads): No contamination detected, majority consensus clean

**Key insight**: The real danger is **missing contamination**, not creating artifactual sequences. When contamination is undetected, the consensus represents the majority population accurately.

**Interpretation**:
- **Clean separation** (two clusters with correct sizes): Pipeline successfully detected contamination
- **Franken-consensus** (one cluster with mixed reads): Sequences too similar or algorithm too aggressive
- **Multiple small clusters**: Over-fragmentation, may need parameter tuning
- Use `--min-cluster-ratio` to filter contaminants below a certain proportion
- Check quality_report.txt (from speconsense-summarize) to identify mixed consensuses by elevated stability metrics

**Example analysis**:
```bash
# Check cluster separation
echo "=== MCL 95/5 ==="
grep "^>" mcl_95_5/contam_95_5-all.fasta

# Expected clean separation (distantly-related contaminants):
# >contam_95_5-c1 size=380 ric=100 p50diff=0.0 p95diff=0.0
# >contam_95_5-c2 size=20 ric=20
# (May also see small artifact clusters size ≤4)

echo "=== Greedy 95/5 ==="
grep "^>" greedy_95_5/contam_95_5-all.fasta

# If contamination is masked (greedy at low contamination):
# >contam_95_5-c1 size=400 ric=100 p50diff=0.0 p95diff=0.0
# (Single cluster, perfect stability, minority ignored)

echo "=== Greedy 50/50 (franken-consensus test) ==="
grep "^>" greedy_50_50/contam_50_50-all.fasta

# If you see this, it's a franken-consensus:
# >contam_50_50-c1 size=400 ric=100 p50diff=5.0 p95diff=12.0
# (Single cluster, elevated p50diff warns of mixed population)

# For production use, filter minor contaminants
speconsense contam_95_5.fastq --min-cluster-ratio 0.10 -o filtered
# Only keeps clusters ≥10% of total (filters out <10% contamination)
```

**Optional: Test read depth effects on detection**:
```bash
# Test if lower read counts affect contamination detection
# For 5% contamination: N=100 yields 5 minority reads, N=50 yields ~2-3
speconsense-synth target_and_contaminant.fasta -n 100 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5_n100.fastq
speconsense-synth target_and_contaminant.fasta -n 50 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5_n50.fastq

speconsense contam_95_5_n100.fastq --algorithm graph -o mcl_95_5_n100 --min-cluster-ratio 0
speconsense contam_95_5_n50.fastq --algorithm graph -o mcl_95_5_n50 --min-cluster-ratio 0

# Compare: Does N=50 still detect the 5% minority?
grep "^>" mcl_95_5_n100/contam_95_5_n100-all.fasta
grep "^>" mcl_95_5_n50/contam_95_5_n50-all.fasta
# The ~5 minority read threshold means N=100 should detect, N=50 may not
```

This experiment helps you:
1. Understand contamination detection limits (both algorithm and read depth effects)
2. Set appropriate `--min-cluster-ratio` thresholds
3. Recognize franken-consensus signatures in real data (greedy at ~50% contamination)
4. Understand when contamination is masked (greedy at low levels, MCL below ~5 minority reads)
5. Evaluate whether small variants are real biology or contamination

## Interpreting Results

### Understanding Stability Metrics

Stability metrics (p50diff, p95diff) are calculated by generating 100 subsampled consensuses and measuring edit distances. See [Quality Assessment and Reporting](../README.md#quality-assessment-and-reporting) for details.

**In synthetic data**:
- **p50diff=0, p95diff=0**: Perfect consensus, error rate well-controlled
- **p50diff=0, p95diff=1-2**: Excellent consensus with rare outliers
- **p50diff>0**: Systematic heterogeneity - either error rate too high, multiple variants mixed, or contamination
- **High values in single-reference tests**: Suggests error rate is too high or read depth too low

### Identifying Franken-Consensuses

A franken-consensus is created when reads from multiple biological sources get clustered together. Signs include:

1. **Elevated stability metrics**: p50diff ≥2 suggests mixed population
2. **Cluster size discrepancy**: Large cluster (size) but moderate RiC with high variation
3. **Intermediate sequences**: BLAST hits to multiple different species/variants
4. **Known ground truth**: In synthetic tests, compare consensus to input references

### Real Variants vs Artifacts

Use these criteria to distinguish real biology from bioinformatic artifacts:

**Likely real variants**:
- Large cluster size (RiC ≥50)
- Low stability metrics (p50diff=0, p95diff <2)
- Consistent across multiple specimens
- Makes biological sense (e.g., known polymorphic locus)

**Likely artifacts**:
- Small cluster size (RiC <20)
- Intermediate between other variants
- Only appears at high error rates in synthetic tests
- Elevated stability metrics despite moderate size

**Ambiguous cases**:
- Medium cluster size (RiC 20-50)
- Appears inconsistently across specimens
- Could be low-frequency real variant or recurring artifact
- Requires additional evidence (Sanger sequencing, independent confirmation)

### Understanding Contamination Detection Patterns

Synthetic testing reveals important patterns about how contamination is detected (or missed) depending on algorithm choice, contamination level, and read depth.

**When contamination is detected:**
- **Multiple clusters from single specimen**: Clear indication of contamination or biological heterogeneity
- **Both consensuses have high identity** (≥99.5%): Successfully separated biological sources
- **MCL typically succeeds** when minority population has ≥5 reads

**When contamination is masked (silent failure):**
- **Greedy at <20% contamination**: Creates perfect majority consensus (100% identity, p50diff=0)
  - No errors introduced into consensus
  - No warning signals
  - Minority population simply ignored
  - Would pass all quality checks
- **MCL below ~5 minority reads**: Contamination goes undetected
  - Creates accurate majority consensus
  - No artifacts, but incomplete picture

**When franken-consensus forms (detectable failure):**
- **Greedy at ~50% contamination of closely-related sequences**:
  - Creates artifactual consensus with reduced identity (e.g., 96% when both sources are >99%)
  - **WARNING SIGNAL**: Elevated p50diff (e.g., >10)
  - Would fail quality thresholds (below 99.5% match)
  - Easier to detect than silent masking

**Critical insights:**

1. **Single cluster does NOT guarantee clean specimen**
   - Could indicate: (a) clean specimen, (b) contamination below detection threshold, or (c) algorithm masking minority
   - Cross-check with specimen metadata and collection conditions
   - Consider read depth (RiC) when interpreting

2. **Read depth affects detection sensitivity**
   - N≥100: Can detect ≥5% contamination with MCL
   - N≥50: Can detect ≥10% contamination with MCL
   - N<50: Only detects ≥20% contamination with MCL
   - **Absolute minority read count** (~5 minimum) matters more than percentage

3. **Algorithm selection matters**
   - **MCL (default)**: Safer for contamination scenarios, detects when ≥5 minority reads present
   - **Greedy**: Faster but masks realistic contamination levels (<20%) without warning
   - Only use greedy when contamination is impossible

4. **Most important: Consensus quality vs detection**
   - Across tested scenarios, artifactual sequences only appeared with greedy at ~50% contamination
   - More common scenario: contamination is **missed entirely** but consensus represents majority accurately
   - The danger is incomplete information, not necessarily incorrect sequences

**For quality control:**
- Flag specimens with RiC <50 as having limited contamination detection
- Use MCL algorithm for field-collected specimens (contamination always possible)
- Investigate multiple clusters carefully (could be contamination or real biological variation)
- Don't assume single cluster = single biological source

### Limitations of Synthetic Testing

Remember that `speconsense-synth` is a simplified model:

**What it does well**:
- Test consensus quality at controlled error rates
- Demonstrate clustering behavior with known ground truth
- Identify parameter sensitivities
- Provide confidence bounds on expected behavior

**What it cannot model**:
- **Systematic errors**: Real Nanopore errors are not random (homopolymer biases, GC effects, position-specific errors)
- **Chimeric reads**: PCR artifacts joining two templates
- **Heteroduplexes**: DNA strands from different sources annealing together
- **Biological complexity**: Mixed infections, heterozygosity, within-specimen variation
- **Real error distributions**: Synth uses equal error types; reality has more deletions

**The bottom line**: Synthetic experiments provide useful bounds and help you understand algorithm behavior, but definitive answers about variant authenticity will only emerge from applying the full phased-variant toolchain to large numbers of real specimens and observing patterns over time.

## Next Steps

After exploring these synthetic scenarios, you can:

1. **Apply insights to real data**: Use stability metrics and cluster size patterns learned from synthetic tests to evaluate real consensus sequences
2. **Set evidence-based thresholds**: Calibrate `--min-ric`, `--min-size`, and `--min-cluster-ratio` based on synthetic experiments
3. **Design custom tests**: Create synthetic datasets specific to your questions (e.g., specific polymorphic regions, chimera simulation)
4. **Build confidence**: Use repeated synthetic experiments with different seeds to understand variance in outcomes
5. **Review quality reports**: Compare patterns in synthetic quality_report.txt to real data reports to identify anomalies

For more on variant merging and phasing limitations, see [Understanding RiC and Merging](understanding-ric-and-merging.md).

For information on customizing output fields in FASTA headers, see [Customizing FASTA Headers](customizing-fasta-headers.md).
