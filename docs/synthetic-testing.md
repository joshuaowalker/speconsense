# Testing with Speconsense-Synth

This guide explains how to use `speconsense-synth` for empirical testing of consensus quality, variant detection, and understanding the behavior of the Nanopore + Specimux + Speconsense toolchain.

## Quick Reference

**Key Thresholds (based on current empirical testing):**
- **Modern chemistry (1-2% error)**: N≥3 for reliable consensus (≥99.5% identity)
- **Conservative/older chemistry (5-10%)**: N≥4 for reliable consensus at 5% error
- **Contamination detection**: N≥50 for 10% sensitivity, N≥100 for 5% sensitivity
- **Stability metrics**: N≥20 for meaningful p50diff/p95diff calculations
- **Use MCL for field samples**: Greedy silently masks <20% contamination
- **Small clusters (≤4 reads)**: Usually sequencing error artifacts

**Algorithm Selection:**
- **MCL (graph-based, default)**: Detects contamination when ≥5 minority reads present; safer for real specimens
- **Greedy**: Faster but creates franken-consensus at ~50% contamination or silently masks <20% contamination

**Important Caveat:** These findings come from limited in silico testing with fungal ITS sequences (500-600bp) and synthetic error models. Real-world patterns are still emerging. Many default parameters are intentionally conservative pending broader validation. See [Limitations of Empirical Findings](#limitations-of-empirical-findings) below.

## Glossary

- **RiC (Reads in Cluster)**: The number of unique reads supporting a consensus sequence. Higher RiC generally means more confident consensus and better contamination detection sensitivity.
- **Franken-consensus**: An artifactual sequence created when reads from multiple biological sources are incorrectly merged into one cluster. Usually has elevated p50diff as a warning signal.
- **p50diff / p95diff**: Stability metrics measuring edit distance variation across 100 subsampled consensuses. Low values (0-2) indicate stable, homogeneous read pools. Higher values suggest mixed populations or high error rates.
- **Error artifacts**: Small clusters (typically ≤4 reads) formed by random sequencing errors during clustering, not representing real biological variants.
- **Silent masking**: When an algorithm (especially greedy) creates a perfect majority consensus while completely ignoring a minority population, with no warning signals (p50diff=0).

## What is Speconsense-Synth?

`speconsense-synth` is a synthetic read generator that creates simulated Oxford Nanopore reads from reference sequences with controlled error rates. It allows you to generate test datasets where you know the ground truth, enabling systematic exploration of how the bioinformatics pipeline handles different scenarios.

This is an exploratory tool designed to help users and developers build confidence in results by empirically testing specific hypotheses about consensus generation, variant calling, and artifact detection. While it cannot answer all questions definitively (real biological data is complex), it provides valuable bounds on expected behavior and helps identify potential issues.

## Questions Speconsense-Synth Can Help Answer

The reality of variant detection and consensus quality will only emerge over time as we apply the toolchain to large numbers of specimens and patterns become clear. In the meantime, `speconsense-synth` allows us to investigate important questions through controlled experiments:

**Reliability and Quality**
- How reliable are the consensus sequences generated by the Nanopore + Specimux + Speconsense pipeline?
- Given Nanopore's relatively higher per-base error rate, but assuming errors are randomly distributed, how serious is this problem in practice?
- How many reads do we need to get a reliable consensus given a certain error rate?
- What do the stability metrics (p50diff, p95diff) tell us about consensus quality?

**Variant Detection**
- Are all variants identified by Speconsense biologically real, or are some of them bioinformatic artifacts?
- When Speconsense produces many variants for a single specimen that differ only slightly or are intermediate between each other, are these intermediates just artifacts created by mixing reads from a few biologically real variants?
- How do different clustering algorithms (MCL vs greedy) handle variant separation?

**Contamination and Mixing**
- What does a naive consensus combining reads from two different biological variants look like?
- What does a consensus contaminated with reads from laboratory or bioinformatic contamination look like?
- At what contamination levels does clustering successfully separate sequences vs create "franken-consensuses"?

These experiments provide confidence intervals and error bars on current methods while we wait for real-world patterns to emerge from production data.

## Understanding Nanopore Error Characteristics

To design realistic synthetic tests, it's important to understand typical error rates and patterns in Oxford Nanopore sequencing.

### Current Technology (R10.4.1 + V14 Chemistry)

Modern Oxford Nanopore technology achieves [1,2]:
- **Raw read accuracy**: 98.7-99.4% (marketed as "Q20+ chemistry")
- **Overall error rate**: ~0.6-1.3% per base
- **Modal accuracy**: >99% for most reads

For **realistic testing with modern chemistry**, use error rates of **0.01-0.02** (1-2%).

### Older Technology (R9.4 and Earlier)

Historical Nanopore sequencing had higher error rates [3]:
- **R9.4 chemistry**: ~92-97% accuracy, meaning **3-8% error rate**
- **GC-dependent variation**: Low-GC regions ~6% error, high-GC regions ~8% error
- **Modal accuracy**: ~96.8% (Q15), compared to R10.4's ~99.2% (Q21)

For **conservative testing or older data**, use error rates of **0.05-0.10** (5-10%).

### Error Type Distribution

Nanopore errors are not evenly distributed across error types [3]:
- **Deletions**: Most common, accounting for 1.6-2.7% of bases
- **Mismatches (substitutions)**: 1.2-2.2% of bases
- **Insertions**: 1.1-2.4% of bases

**Important patterns** [3]:
- Homopolymer regions and short tandem repeats cause ~50% of all sequencing errors
- GC content affects error rates: low-GC regions ~6%, high-GC regions ~8%
- Deletions consistently outnumber other error types

**Note**: `speconsense-synth` currently distributes error types equally (⅓ each: insertion, deletion, substitution) and uniformly across read positions. This is simpler than reality but provides a reasonable first approximation for testing. Real Nanopore data will have more deletions, position-specific biases, and quality degradation at read ends.

### Implications for Testing

When designing synthetic experiments:
1. **Modern workflows**: Test with 0.01-0.02 error rates to match current R10.4.1 chemistry
2. **Conservative bounds**: Use 0.05-0.10 to stress-test the pipeline or match older chemistry
3. **Error distribution**: Remember real data has more deletions and homopolymer issues than synth generates
4. **Random vs systematic**: Synth errors are random; real errors may be systematic (same position across reads)

## Worked Examples

### 1. Error Rate Impact on Consensus Quality

**Question**: How do different error rates affect consensus quality and stability metrics?

**Setup**: Generate synthetic reads from a single reference at varying error rates and examine the resulting consensus quality.

```bash
# Create a reference sequence (or use existing FASTA)
cat > reference.fasta <<EOF
>ITS_reference
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
GTTTCTATATGTAACTTCTGAGTAAAACCATAAATAAATCAAAACTTTCAACAACGGATCTCTTGGT
EOF

# Generate reads at different error rates
speconsense-synth reference.fasta -n 200 -e 0.01 --seed 42 -o reads_1pct.fastq
speconsense-synth reference.fasta -n 200 -e 0.05 --seed 42 -o reads_5pct.fastq
speconsense-synth reference.fasta -n 200 -e 0.10 --seed 42 -o reads_10pct.fastq

# Run speconsense on each dataset
speconsense reads_1pct.fastq -o test_1pct --min-size 0
speconsense reads_5pct.fastq -o test_5pct --min-size 0
speconsense reads_10pct.fastq -o test_10pct --min-size 0

# Examine the consensus sequences and stability metrics
grep "^>" test_1pct/reads_1pct-all.fasta
grep "^>" test_5pct/reads_5pct-all.fasta
grep "^>" test_10pct/reads_10pct-all.fasta
```

**What to look for**:
- **1% error rate**: Should produce a single consensus with excellent stability (p50diff=0, p95diff close to 0)
- **5% error rate**: May show elevated p95diff values, indicating some subsampling variation
- **10% error rate**: Higher stability metrics, potential for spurious variant clusters

**Observed in practice**:
Using real fungal ITS sequences with N=200 reads in silico:
- **1-2% error** (modern R10.4.1 chemistry): 100% consensus identity, p50diff=0, p95diff=0
- **5% error** (conservative/older chemistry): 99.85% consensus identity, p50diff=0, p95diff=1
- **10% error** (stress test): 99.85% consensus identity, p50diff=0, p95diff=1

Even at 10% error rate, consensus quality remains above the 99.5% biological match threshold in our testing. Modern error rates (1-2%) produce essentially perfect consensus sequences for these test cases.

**Interpretation**:
- Low p50diff values (0-1) indicate the consensus is stable across subsampling
- Elevated p50diff (≥2) suggests systematic heterogeneity in the read pool
- Compare consensus sequence to reference to measure accuracy
- Count the number of variants: multiple clusters from a single reference indicate error-driven over-clustering

**Example expected output**:
```
# 1% error rate - excellent quality
>reads_1pct-c1 size=200 ric=100 p50diff=0.0 p95diff=0.0

# 5% error rate - good quality, some variation
>reads_5pct-c1 size=195 ric=100 p50diff=0.0 p95diff=2.0
>reads_5pct-c2 size=5 ric=5 p50diff=1.0 p95diff=3.0

# 10% error rate - degraded quality, possible spurious variants
>reads_10pct-c1 size=180 ric=100 p50diff=1.0 p95diff=4.0
>reads_10pct-c2 size=15 ric=15 p50diff=2.0 p95diff=5.0
>reads_10pct-c3 size=5 ric=5 p50diff=3.0 p95diff=6.0
```

This experiment helps you understand: (1) what stability metrics mean in practice, (2) how error rates affect clustering behavior, and (3) what quality thresholds are reasonable for your data.

### 2. Detecting Artifact Variants from Mixed Populations

**Question**: When reads from multiple biological variants are present, does Speconsense create intermediate "phantom" variants that don't actually exist biologically?

**Setup**: Create two variants differing by 2-3 SNPs, mix their reads, and look for intermediate sequences.

```bash
# Create two variants that differ at specific positions
cat > two_variants.fasta <<EOF
>variant_A
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
>variant_B
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTGAAACGGGACGGCCCGCCGGAGGACCCCTAAACTCT
EOF
# variant_B differs at 2 positions (marked above with differences)

# Mix variants 50/50 with different error rates
speconsense-synth two_variants.fasta -n 400 -e 0.02 --ratios 50,50 --seed 42 -o mixed_2pct.fastq
speconsense-synth two_variants.fasta -n 400 -e 0.05 --ratios 50,50 --seed 42 -o mixed_5pct.fastq
speconsense-synth two_variants.fasta -n 400 -e 0.10 --ratios 50,50 --seed 42 -o mixed_10pct.fastq

# Cluster with both algorithms
# Note: Use --min-cluster-ratio 0 to see ALL clusters including small artifacts
speconsense mixed_2pct.fastq --algorithm graph -o mcl_2pct --min-size 0 --min-cluster-ratio 0
speconsense mixed_2pct.fastq --algorithm greedy -o greedy_2pct --min-size 0 --min-cluster-ratio 0

# Test higher error rates with MCL
speconsense mixed_5pct.fastq --algorithm graph -o mcl_5pct --min-size 0 --min-cluster-ratio 0
speconsense mixed_10pct.fastq --algorithm graph -o mcl_10pct --min-size 0 --min-cluster-ratio 0

# Examine the output
grep "^>" mcl_2pct/mixed_2pct-all.fasta

# Run summarize to see if SNP merging creates the expected merged sequence
speconsense-summarize --source mcl_2pct --summary-dir mcl_2pct_summary --min-ric 5
```

**What to look for**:
- **Expected**: Two main clusters corresponding to variants A and B
- **Artifact detection**: Check for small intermediate clusters with sequences that have one SNP from A and one from B (phantom variants)
- **Algorithm comparison**: MCL may produce more granular variants; greedy may merge more aggressively
- **SNP merging**: `speconsense-summarize` may merge the two variants if they differ by ≤2 SNPs (controlled by `--merge-position-count`)

**Observed in in silico testing**:
Testing with closely-related *Amanita* variants (99.5% identity, 3 SNPs apart) at 2% error:
- **MCL**: Successfully separates both variants (100% identity to each reference). Additional small clusters (size ≤4) are sequencing error artifacts, NOT biological intermediates from mixing.
- **Greedy**: Algorithm behavior depends on mixing ratio:
  - 50/50 mix: Creates a franken-consensus (see Example 4 for details)
  - Unequal ratios: May separate or merge depending on distance threshold

**Key finding on artifacts**: When examining clusters with `--min-cluster-ratio 0` in our in silico testing, all small clusters (≤4 reads) were sequencing error artifacts, clearly matching one parent variant or the other (~99% to one, ~92% to the other). None showed intermediate identity suggesting biological mixing.

**Interpretation**:
- Intermediate variants with small size (RiC <20) are likely artifacts from error-driven mixing
- Intermediate variants with large size suggest real heterogeneity or contamination
- Higher error rates increase artifact formation
- Use `--min-size` and `--min-cluster-ratio` to filter small artifact clusters
- Check merged sequences in summary output: if variants A and B merge into one IUPAC consensus, this represents the phasing loss discussed in [Understanding RiC and Merging](understanding-ric-and-merging.md)

**Example investigation**:
```bash
# Extract consensus sequences and align to reference variants
# Look for intermediate sequences (one SNP from each parent)
# Count cluster sizes to identify likely artifacts vs real variants

# Check if speconsense-summarize merged them
grep "rawric" mcl_2pct_summary/*.fasta
# Output like: >sample-1 size=400 ric=200 rawric=200+200 snp=2
# This indicates two variants merged with IUPAC codes (ambiguity)
```

This experiment demonstrates the phasing problem and helps calibrate your expectation for when variants are real vs artifacts.

### 3. Read Depth Requirements for Reliable Consensus

**Question**: How many reads are needed to generate a reliable consensus sequence (≥99.5% identity to reference) at a given error rate?

**Setup**: Generate varying numbers of reads from a single reference and measure consensus identity. Use granular stepping at low read counts to find the exact threshold where consensus becomes reliable.

```bash
# Create reference
cat > reference.fasta <<EOF
>test_sequence
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
EOF

# Test at different error rates with granular N stepping
# Modern chemistry (1-2% error) - test very low N
for N in 2 3 4 5 6 7 8 9 10 15 20; do
    speconsense-synth reference.fasta -n $N -e 0.02 --seed 42 -o reads_1pct_n${N}.fastq
    speconsense reads_1pct_n${N}.fastq -o test_1pct_n${N} --min-size 0
done

# Older chemistry (5% error) - test low to medium N
for N in 2 3 4 5 6 7 8 9 10 15 20 30 50; do
    speconsense-synth reference.fasta -n $N -e 0.05 --seed 42 -o reads_5pct_n${N}.fastq
    speconsense reads_5pct_n${N}.fastq -o test_5pct_n${N} --min-size 0
done

# Extreme error (10%) - test higher N
for N in 20 30 40 50 75 100; do
    speconsense-synth reference.fasta -n $N -e 0.10 --seed 42 -o reads_10pct_n${N}.fastq
    speconsense reads_10pct_n${N}.fastq -o test_10pct_n${N} --min-size 0
done

# Compare consensus identity to reference
# (Use edlib or similar alignment tool to calculate % identity)
```

**What to look for**:
- **Threshold N:** At what read count does consensus identity cross ≥99.5%?
- **Perfect consensus:** At what N does identity reach 100%?
- **Failure modes:** Can consensus be generated at very low N (N<5)?
- **Error rate effect:** How much does error rate change the minimum N requirement?

**Observed in practice**:

In silico testing with *Russula* sp. ITS sequence (599 bp) using granular N stepping from 1 to 100:

**Modern chemistry (1-2% error):**
- **N=1:** No consensus (cannot cluster single read)
- **N=2:** 99.33% identity (just below threshold)
- **N=3:** 99.83% identity ✓ (crosses 99.5% threshold)
- **N≥4:** 100% identity (perfect consensus in tested cases)

**Older chemistry (5% error):**
- **N=1:** No consensus
- **N=2:** 96.56% identity (poor quality)
- **N=3:** 99.00% identity (close but below threshold)
- **N=4:** 99.67% identity ✓ (crosses 99.5% threshold)
- **N≥7:** 100% identity consistently in tests

**Extreme error (10%):**
- **N≤15:** No consensus (clustering fails, reads too divergent)
- **N=20:** 91.08% identity (consensus generated but poor quality)
- **N=30:** 97.84% identity (approaching threshold)
- **N=40:** 99.83% identity ✓ (crosses 99.5% threshold)
- **N≥50:** 100% identity in tested cases

**Key finding:** With modern ONT chemistry (1-2% error), **as few as 3-4 reads** can produce a reliable consensus (≥99.5% identity) in our testing with fungal ITS sequences. This is much lower than previous estimates based on stability metrics, though these results need validation with other sequence types.

**Read depth for different purposes:**

Reliable consensus and other quality metrics have **different read depth requirements** (based on our fungal ITS testing):

| Purpose | Minimum N | Reason |
|---------|-----------|--------|
| Reliable consensus (≥99.5% identity) | 3-4 | Error correction via voting |
| Stability calculation (p50diff, p95diff) | 20+ | Subsampling needs sufficient reads |
| Detect 10% contamination | 50+ | Needs ≥5 minority reads |
| Detect 5% contamination | 100+ | Needs ≥5 minority reads |

**Implication:** A specimen with RiC=10 might have:
- ✓ Reliable consensus of majority population (100% identity)
- ✗ Insufficient reads for stability assessment (p50diff unavailable or unreliable)
- ✗ Insufficient for contamination detection below 20%

This explains why single clusters don't guarantee clean specimens - contamination may be present but below detection threshold!

See Example 4 for details on contamination detection across read depths.

**Interpretation**:
- **RiC <3:** Consensus unreliable, discard
- **RiC 3-5:** Consensus likely reliable (modern chemistry) but no stability metrics, limited contamination detection
- **RiC 5-20:** Good consensus quality, marginal stability assessment, detects >20% contamination
- **RiC 20-50:** Reliable consensus + stable metrics, detects >10% contamination
- **RiC ≥50:** Excellent quality, good contamination detection (≥10%)
- **RiC ≥100:** Sensitive contamination detection (≥5%)

**Expected results summary**:

```
Read Depth vs Consensus Identity

   N |   1% error  |   5% error  |  10% error
-----|-------------|-------------|-------------
   2 |   99.33%    |   96.56%    |    FAIL
   3 | ✓ 99.83%    |   99.00%    |    FAIL
   4 | ✓ 100%      | ✓ 99.67%    |    FAIL
   5 | ✓ 100%      | ✓ 99.83%    |    FAIL
   7 | ✓ 100%      | ✓ 100%      |    FAIL
  10 | ✓ 100%      | ✓ 100%      |    FAIL
  20 | ✓ 100%      | ✓ 100%      |   91.08%
  30 | ✓ 100%      | ✓ 100%      |   97.84%
  40 | ✓ 100%      | ✓ 100%      | ✓ 99.83%
  50 | ✓ 100%      | ✓ 99.83%    | ✓ 100%
 100 | ✓ 100%      | ✓ 99.83%    | ✓ 100%

Minimum N for ≥99.5% identity:
  1% error: N ≥ 3 reads
  5% error: N ≥ 4 reads
 10% error: N ≥ 40 reads
```

This experiment helps you:
1. Understand the remarkably low read requirements for reliable consensus (3-4 reads for modern chemistry)
2. Recognize that consensus quality and other purposes (stability, contamination detection) have different read depth needs
3. Set appropriate RiC filtering thresholds based on intended use
4. Understand why low-RiC specimens can have good consensus but limited contamination detection

### 4. Contamination Scenarios

**Question**: What happens when reads from a contaminant sequence mix with the target sequence? Do they cluster separately or create a franken-consensus?

**Setup**: Mix a target sequence with a contaminant at varying ratios and examine clustering behavior.

```bash
# Create target and contaminant sequences (e.g., similar but distinct species)
cat > target_and_contaminant.fasta <<EOF
>target_species
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
ATGTTGCCTCGGCGGATCAGCCCGCTCCCGGTAAAACGGGACGGCCCGCCAGAGGACCCCTAAACTCT
>contaminant
TCCGTAGGTGAACCTGCGGAAGGATCATTACCGAGTTTACAACTCCCAAACCCCTGTGAACATACCTA
GTGAACATACCTAATGTTGCCTCGGCGGATCAGCCCGCCCGGCCCGCCAGAGGACCCCTAAACTCTGT
EOF
# Contaminant differs significantly (different species/genus)

# Test different contamination levels
# 95/5 ratio - minor contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5.fastq

# 90/10 ratio - moderate contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 90,10 --seed 42 -o contam_90_10.fastq

# 80/20 ratio - heavy contamination
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 80,20 --seed 42 -o contam_80_20.fastq

# Test with both clustering algorithms
# Note: Use --min-cluster-ratio 0 to see ALL clusters including small artifacts
speconsense contam_95_5.fastq --algorithm graph -o mcl_95_5 --min-size 0 --min-cluster-ratio 0
speconsense contam_95_5.fastq --algorithm greedy -o greedy_95_5 --min-size 0 --min-cluster-ratio 0

# Test additional ratios
speconsense contam_90_10.fastq --algorithm graph -o mcl_90_10 --min-size 0 --min-cluster-ratio 0
speconsense contam_80_20.fastq --algorithm graph -o mcl_80_20 --min-size 0 --min-cluster-ratio 0

# Compare algorithm behavior at equal mixture (50/50)
# This tests the franken-consensus danger zone
speconsense-synth target_and_contaminant.fasta -n 400 -e 0.02 \
    --ratios 50,50 --seed 42 -o contam_50_50.fastq
speconsense contam_50_50.fastq --algorithm graph -o mcl_50_50 --min-size 0 --min-cluster-ratio 0
speconsense contam_50_50.fastq --algorithm greedy -o greedy_50_50 --min-size 0 --min-cluster-ratio 0
```

**What to look for**:
- **Separation**: Do you get two clean clusters (target + contaminant)?
- **Cluster sizes**: Do they match expected ratios (e.g., ~380 vs ~20 for 95/5)?
- **Franken-consensus**: At what contamination level do sequences start mixing into chimeric consensuses?
- **Algorithm differences**: Does graph (MCL) separate better than greedy?
- **Small artifacts**: Do you see tiny clusters (size ≤4) that are error artifacts rather than real variants?
- **Read depth effects**: How does total read count affect detection of minority population?

**Observed in practice**:

In silico testing of contamination scenarios reveals distinct algorithm behaviors and critical detection limits:

**1. Distantly-related contamination (e.g., different genera at ~65% identity)**:
- MCL detects cleanly at all tested ratios (95/5, 90/10, 80/20)
- Both consensuses achieve 99.85-100% identity to references
- Some error artifact clusters (size ≤4) appear, especially at higher contamination
- Greedy behavior varies by ratio (see below)

**2. Closely-related contamination (~92% identity, same genus)**:
This is the challenging "worst case" scenario revealing critical algorithm differences in our testing:

**MCL Algorithm (graph-based):**
- ✓ Successfully detects both variants at all contamination levels **when minority has ≥5 reads**
- ✓ All main consensuses: 99.66-100% identity to references
- ✓ Read depth requirements for detection:
  - 5% contamination: Requires N≥100 (to yield 5 minority reads)
  - 10% contamination: Requires N≥50 (to yield 5 minority reads)
  - 20% contamination: Detectable at N≥25
- **Critical finding**: Detection depends on **absolute minority read count (~5 minimum)**, not just percentage

**Greedy Algorithm:**
Behavior changes dramatically based on contamination level:

- **At ~50% contamination (equal mixture)**:
  - Creates **franken-consensus** (e.g., 96.40% identity when both parents are >99%)
  - **WARNING SIGNAL**: Elevated p50diff (e.g., 18.0) indicates heterogeneity
  - Sequence falls below 99.5% threshold → would fail quality checks
  - This is detectable (doesn't pass as good data)

- **At <20% contamination (realistic contamination levels)**:
  - Creates perfect **majority consensus** (100% identity to majority variant)
  - **NO WARNING SIGNAL**: p50diff=0, appears perfectly stable
  - Minority population completely ignored
  - Would pass all quality checks despite contamination present

- **Transition zone at ~20-40% contamination**: Behavior is unpredictable

**3. Most Important Characteristic - When Artifacts Appear in Consensus**:

Across all tested scenarios, **artifacts appearing in the consensus sequence** only occurred with:
- Greedy algorithm at ~50% contamination of closely-related sequences
- This creates detectable franken-consensus (high p50diff, low identity)

**Silent failures** (contamination present but consensus appears clean):
- Greedy at <20% contamination: Creates clean majority consensus, no errors, no warnings
- MCL below detection threshold (<5 minority reads): No contamination detected, majority consensus clean

**Key insight**: The real danger is **missing contamination**, not creating artifactual sequences. When contamination is undetected, the consensus represents the majority population accurately.

**Interpretation**:
- **Clean separation** (two clusters with correct sizes): Pipeline successfully detected contamination
- **Franken-consensus** (one cluster with mixed reads): Sequences too similar or algorithm too aggressive
- **Multiple small clusters**: Over-fragmentation, may need parameter tuning
- Use `--min-cluster-ratio` to filter contaminants below a certain proportion
- Check quality_report.txt (from speconsense-summarize) to identify mixed consensuses by elevated stability metrics

**Example analysis**:
```bash
# Check cluster separation
echo "=== MCL 95/5 ==="
grep "^>" mcl_95_5/contam_95_5-all.fasta

# Expected clean separation (distantly-related contaminants):
# >contam_95_5-c1 size=380 ric=100 p50diff=0.0 p95diff=0.0
# >contam_95_5-c2 size=20 ric=20
# (May also see small artifact clusters size ≤4)

echo "=== Greedy 95/5 ==="
grep "^>" greedy_95_5/contam_95_5-all.fasta

# If contamination is masked (greedy at low contamination):
# >contam_95_5-c1 size=400 ric=100 p50diff=0.0 p95diff=0.0
# (Single cluster, perfect stability, minority ignored)

echo "=== Greedy 50/50 (franken-consensus test) ==="
grep "^>" greedy_50_50/contam_50_50-all.fasta

# If you see this, it's a franken-consensus:
# >contam_50_50-c1 size=400 ric=100 p50diff=5.0 p95diff=12.0
# (Single cluster, elevated p50diff warns of mixed population)

# For production use, filter minor contaminants
speconsense contam_95_5.fastq --min-cluster-ratio 0.10 -o filtered
# Only keeps clusters ≥10% of total (filters out <10% contamination)
```

**Optional: Test read depth effects on detection**:
```bash
# Test if lower read counts affect contamination detection
# For 5% contamination: N=100 yields 5 minority reads, N=50 yields ~2-3
speconsense-synth target_and_contaminant.fasta -n 100 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5_n100.fastq
speconsense-synth target_and_contaminant.fasta -n 50 -e 0.02 \
    --ratios 95,5 --seed 42 -o contam_95_5_n50.fastq

speconsense contam_95_5_n100.fastq --algorithm graph -o mcl_95_5_n100 --min-cluster-ratio 0
speconsense contam_95_5_n50.fastq --algorithm graph -o mcl_95_5_n50 --min-cluster-ratio 0

# Compare: Does N=50 still detect the 5% minority?
grep "^>" mcl_95_5_n100/contam_95_5_n100-all.fasta
grep "^>" mcl_95_5_n50/contam_95_5_n50-all.fasta
# The ~5 minority read threshold means N=100 should detect, N=50 may not
```

This experiment helps you:
1. Understand contamination detection limits (both algorithm and read depth effects)
2. Set appropriate `--min-cluster-ratio` thresholds
3. Recognize franken-consensus signatures in real data (greedy at ~50% contamination)
4. Understand when contamination is masked (greedy at low levels, MCL below ~5 minority reads)
5. Evaluate whether small variants are real biology or contamination

## Limitations of Empirical Findings

The empirical findings presented in this guide represent **initial exploratory in silico research** based on limited synthetic testing. The landscape of variant detection, contamination patterns, and consensus quality is **actively evolving** as we accumulate more real-world data.

### Scope of Current Testing

**Test sequences used**:
- Primarily fungal ITS sequences from real ONT barcoding runs
- Sequence lengths: 500-600bp (typical for ITS region)
- Test organisms: *Russula* sp., *Amanita* sp., *Pleurotus* sp.
- Sample size: Small number of reference sequences (3-5 sequences per experiment type)
- Replication: Single-seed in silico tests for most findings, limited cross-validation

**Experimental parameters:**
- Error rates: 1%, 2%, 5%, 10% (synthetic, uniformly distributed)
- Read depths: Granular testing from N=2 to N=400
- Contamination ratios: 50/50, 80/20, 90/10, 95/5
- Sequence divergences tested: 99.5% identity (closely related), 92% identity (same genus), 65% identity (different genera)

### What These Findings Can and Cannot Tell You

**What is supported by current in silico testing:**
- ✓ Relative performance of MCL vs greedy algorithms on fungal ITS sequences
- ✓ General trends in read depth requirements for modern chemistry error rates
- ✓ Qualitative contamination detection patterns (when detected vs masked)
- ✓ Approximate thresholds for error artifact cluster sizes (≤4 reads)
- ✓ Demonstration that very low read counts (N=3-4) can produce reliable consensus with modern chemistry

**What requires further validation:**
- ⚠ Exact threshold values (e.g., "≥5 minority reads", "≤4 = artifacts") may vary with sequence type
- ⚠ Applicability to non-ITS regions (different lengths, structures, or conservation patterns)
- ⚠ Behavior with real systematic Nanopore errors (homopolymer runs, GC bias) vs synthetic random errors
- ⚠ Applicability to non-fungal organisms (bacterial 16S, animal COI, etc.)
- ⚠ Edge cases and failure modes not yet encountered
- ⚠ Optimal parameter combinations for specific use cases

### Why Default Parameters Are Conservative

Given the limited empirical validation to date, **default parameters in Speconsense are intentionally balanced** between sensitivity and specificity:

**Key defaults and their rationale:**
- `--min-size 5` (speconsense): Filters small clusters at the empirically-observed artifact threshold (≤4 reads)
- `--min-ric 3` (speconsense-summarize): Matches the minimum N for reliable consensus shown in testing
- `--min-cluster-ratio 0.2` (speconsense): Filters clusters smaller than 20% of the largest cluster to remove minor contaminants
- MCL as default algorithm: More computationally expensive but safer for contamination detection than greedy

**Rationale:**
- Real biological data is more complex than in silico testing captures
- Systematic errors (homopolymer issues, GC bias) may create different artifact patterns than random errors
- Defaults aim to balance **avoiding false variants** with maintaining reasonable sensitivity
- As real-world patterns emerge from large-scale deployment, parameters may be adjusted where justified

**Evolving recommendations:**
- The guidelines in this document represent **current best understanding** based on limited experiments
- Thresholds and recommendations will be updated as more data becomes available
- Users applying this pipeline to novel sequence types or organisms should validate findings independently
- Consider synthetic testing with your own reference sequences to calibrate parameters

### Variability and Reproducibility

**Stochastic variation:**
- Most findings based on single-seed experiments (seed=42)
- Some variation expected across different random seeds
- Claims of "100% identity" or "always artifacts" should be interpreted as "in tested cases"

**Biological variation:**
- Real specimens may have within-specimen heterogeneity not modeled in synthetic tests
- Mixed infections, heterozygosity, or somatic variation could create patterns different from contamination
- Field-collected specimens have more complex error profiles than synthetic data

**Reproducibility:**
- All in silico examples are reproducible with provided commands and seed values
- Results validated on macOS/Linux platforms

### Recommended Validation for Production Use

If applying Speconsense to a new organism group or sequence type:

1. **Generate synthetic tests** with your reference sequences using this guide
2. **Validate key thresholds**: Test whether N≥3 consensus quality and ≤4 artifact patterns hold
3. **Calibrate contamination detection**: Test with known mixed samples if available
4. **Cross-validate with Sanger**: Sequence a subset of specimens with Sanger to confirm consensus accuracy
5. **Start conservative**: Use default parameters initially, then relax based on observed patterns
6. **Monitor over time**: Track variant patterns across hundreds of specimens to identify systematic issues

### Long-term Perspective

**This is the beginning, not the end:** The definitive answers about variant authenticity, optimal filtering thresholds, and contamination patterns will only emerge as we:
- Process thousands of specimens with known biology
- Observe recurring patterns across diverse datasets
- Validate computational results against orthogonal methods (Sanger, morphology, ecology)
- Accumulate failure cases and edge conditions

**Adaptive approach:** As real-world evidence accumulates, parameters and recommendations will be updated. This guide represents a **snapshot of current understanding** to help users get started with informed expectations, not a final specification.

## Interpreting Results

### Understanding Stability Metrics

Stability metrics (p50diff, p95diff) are calculated by generating 100 subsampled consensuses and measuring edit distances. See [Quality Assessment and Reporting](../README.md#quality-assessment-and-reporting) for details.

**In synthetic data**:
- **p50diff=0, p95diff=0**: Perfect consensus, error rate well-controlled
- **p50diff=0, p95diff=1-2**: Excellent consensus with rare outliers
- **p50diff>0**: Systematic heterogeneity - either error rate too high, multiple variants mixed, or contamination
- **High values in single-reference tests**: Suggests error rate is too high or read depth too low

### Identifying Franken-Consensuses

A franken-consensus is created when reads from multiple biological sources get clustered together. Signs include:

1. **Elevated stability metrics**: p50diff ≥2 suggests mixed population
2. **Cluster size discrepancy**: Large cluster (size) but moderate RiC with high variation
3. **Intermediate sequences**: BLAST hits to multiple different species/variants
4. **Known ground truth**: In synthetic tests, compare consensus to input references

### Real Variants vs Artifacts

Use these criteria to distinguish real biology from bioinformatic artifacts:

**Likely real variants**:
- Large cluster size (RiC ≥50)
- Low stability metrics (p50diff=0, p95diff <2)
- Consistent across multiple specimens
- Makes biological sense (e.g., known polymorphic locus)

**Likely artifacts**:
- Small cluster size (RiC <20)
- Intermediate between other variants
- Only appears at high error rates in synthetic tests
- Elevated stability metrics despite moderate size

**Ambiguous cases**:
- Medium cluster size (RiC 20-50)
- Appears inconsistently across specimens
- Could be low-frequency real variant or recurring artifact
- Requires additional evidence (Sanger sequencing, independent confirmation)

### Understanding Contamination Detection Patterns

In silico synthetic testing with fungal ITS sequences reveals important patterns about how contamination is detected (or missed) depending on algorithm choice, contamination level, and read depth. While these patterns are based on limited testing (see [Limitations of Empirical Findings](#limitations-of-empirical-findings)), they provide useful guidance for interpreting real data.

**When contamination is detected:**
- **Multiple clusters from single specimen**: Clear indication of contamination or biological heterogeneity
- **Both consensuses have high identity** (≥99.5%): Successfully separated biological sources
- **MCL typically succeeds** when minority population has ≥5 reads

**When contamination is masked (silent failure):**
- **Greedy at <20% contamination**: Creates perfect majority consensus (100% identity, p50diff=0)
  - No errors introduced into consensus
  - No warning signals
  - Minority population simply ignored
  - Would pass all quality checks
- **MCL below ~5 minority reads**: Contamination goes undetected
  - Creates accurate majority consensus
  - No artifacts, but incomplete picture

**When franken-consensus forms (detectable failure):**
- **Greedy at ~50% contamination of closely-related sequences**:
  - Creates artifactual consensus with reduced identity (e.g., 96% when both sources are >99%)
  - **WARNING SIGNAL**: Elevated p50diff (e.g., >10)
  - Would fail quality thresholds (below 99.5% match)
  - Easier to detect than silent masking

**Critical insights:**

1. **Single cluster does NOT guarantee clean specimen**
   - Could indicate: (a) clean specimen, (b) contamination below detection threshold, or (c) algorithm masking minority
   - Cross-check with specimen metadata and collection conditions
   - Consider read depth (RiC) when interpreting

2. **Read depth affects detection sensitivity**
   - N≥100: Can detect ≥5% contamination with MCL
   - N≥50: Can detect ≥10% contamination with MCL
   - N<50: Only detects ≥20% contamination with MCL
   - **Absolute minority read count** (~5 minimum) matters more than percentage

3. **Algorithm selection matters**
   - **MCL (default)**: Safer for contamination scenarios, detects when ≥5 minority reads present
   - **Greedy**: Faster but masks realistic contamination levels (<20%) without warning
   - Only use greedy when contamination is impossible

4. **Most important: Consensus quality vs detection**
   - Across tested scenarios, artifactual sequences only appeared with greedy at ~50% contamination
   - More common scenario: contamination is **missed entirely** but consensus represents majority accurately
   - The danger is incomplete information, not necessarily incorrect sequences

**For quality control:**
- Flag specimens with RiC <50 as having limited contamination detection
- Use MCL algorithm for field-collected specimens (contamination always possible)
- Investigate multiple clusters carefully (could be contamination or real biological variation)
- Don't assume single cluster = single biological source

### Limitations of Synthetic Testing

Remember that `speconsense-synth` is a simplified model:

**What it does well**:
- Test consensus quality at controlled error rates
- Demonstrate clustering behavior with known ground truth
- Identify parameter sensitivities
- Provide confidence bounds on expected behavior

**What it cannot model**:
- **Systematic errors**: Real Nanopore errors are not random (homopolymer biases, GC effects, position-specific errors)
- **Read-end quality degradation**: Real Nanopore reads often have contiguous low-quality regions, particularly at read ends, rather than uniformly distributed errors
- **Chimeric reads**: PCR artifacts joining two templates
- **Heteroduplexes**: DNA strands from different sources annealing together
- **Biological complexity**: Mixed infections, heterozygosity, within-specimen variation
- **Real error distributions**: Synth uses equal error types; reality has more deletions

**The bottom line**: Synthetic experiments provide useful bounds and help you understand algorithm behavior, but definitive answers about variant authenticity will only emerge from applying the full phased-variant toolchain to large numbers of real specimens and observing patterns over time.

## Next Steps

After exploring these synthetic scenarios, you can:

1. **Apply insights to real data**: Use stability metrics and cluster size patterns learned from synthetic tests to evaluate real consensus sequences
2. **Set evidence-based thresholds**: Calibrate `--min-ric`, `--min-size`, and `--min-cluster-ratio` based on synthetic experiments
3. **Design custom tests**: Create synthetic datasets specific to your questions (e.g., specific polymorphic regions, chimera simulation)
4. **Build confidence**: Use repeated synthetic experiments with different seeds to understand variance in outcomes
5. **Review quality reports**: Compare patterns in synthetic quality_report.txt to real data reports to identify anomalies

For more on variant merging and phasing limitations, see [Understanding RiC and Merging](understanding-ric-and-merging.md).

For information on customizing output fields in FASTA headers, see [Customizing FASTA Headers](customizing-fasta-headers.md).

## References

[1] Wang Y, Zhao Y, Bollas A, Wang Y, Au KF. The newest Oxford Nanopore R10.4.1 full-length 16S rRNA sequencing enables the accurate resolution of species-level microbial community profiling. *Appl Environ Microbiol*. 2023 Oct 31;89(10):e0060523. doi: 10.1128/aem.00605-23. PMID: 37796026; PMCID: PMC10617388.

[2] Oxford Nanopore Technologies. Nanopore sequencing accuracy. Available at: https://nanoporetech.com/platform/accuracy

[3] Delahaye C, Nicolas J. Sequencing DNA with nanopores: Troubles and biases. *PLoS One*. 2021 Oct 1;16(10):e0257521. doi: 10.1371/journal.pone.0257521. PMID: 34597327; PMCID: PMC8486125.
